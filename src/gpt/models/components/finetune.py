"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
import dataclasses
from .finetuneevent import FineTuneEvent
from .openaifile import OpenAIFile
from dataclasses_json import Undefined, dataclass_json
from enum import Enum
from gpt import utils
from typing import List, Optional


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class Hyperparams:
    r"""The hyperparameters used for the fine-tuning job. See the [fine-tuning guide](/docs/guides/legacy-fine-tuning/hyperparameters) for more details."""
    batch_size: int = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('batch_size') }})
    r"""The batch size to use for training. The batch size is the number of
    training examples used to train a single forward and backward pass.
    """
    learning_rate_multiplier: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('learning_rate_multiplier') }})
    r"""The learning rate multiplier to use for training."""
    n_epochs: int = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('n_epochs') }})
    r"""The number of epochs to train the model for. An epoch refers to one
    full cycle through the training dataset.
    """
    prompt_loss_weight: float = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('prompt_loss_weight') }})
    r"""The weight to use for loss on the prompt tokens."""
    classification_n_classes: Optional[int] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('classification_n_classes'), 'exclude': lambda f: f is None }})
    r"""The number of classes to use for computing classification metrics."""
    classification_positive_class: Optional[str] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('classification_positive_class'), 'exclude': lambda f: f is None }})
    r"""The positive class to use for computing classification metrics."""
    compute_classification_metrics: Optional[bool] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('compute_classification_metrics'), 'exclude': lambda f: f is None }})
    r"""The classification metrics to compute using the validation dataset at the end of every epoch."""
    


class FineTuneObject(str, Enum):
    r"""The object type, which is always \\"fine-tune\\"."""
    FINE_TUNE = 'fine-tune'


@dataclass_json(undefined=Undefined.EXCLUDE)
@dataclasses.dataclass
class FineTune:
    r"""The `FineTune` object represents a legacy fine-tune job that has been created through the API.

    Deprecated class: This will be removed in a future release, please migrate away from it as soon as possible.
    """
    created_at: int = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('created_at') }})
    r"""The Unix timestamp (in seconds) for when the fine-tuning job was created."""
    fine_tuned_model: Optional[str] = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('fine_tuned_model') }})
    r"""The name of the fine-tuned model that is being created."""
    hyperparams: Hyperparams = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('hyperparams') }})
    r"""The hyperparameters used for the fine-tuning job. See the [fine-tuning guide](/docs/guides/legacy-fine-tuning/hyperparameters) for more details."""
    id: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('id') }})
    r"""The object identifier, which can be referenced in the API endpoints."""
    model: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('model') }})
    r"""The base model that is being fine-tuned."""
    object: FineTuneObject = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('object') }})
    r"""The object type, which is always \\"fine-tune\\"."""
    organization_id: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('organization_id') }})
    r"""The organization that owns the fine-tuning job."""
    result_files: List[OpenAIFile] = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('result_files') }})
    r"""The compiled results files for the fine-tuning job."""
    status: str = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('status') }})
    r"""The current status of the fine-tuning job, which can be either `created`, `running`, `succeeded`, `failed`, or `cancelled`."""
    training_files: List[OpenAIFile] = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('training_files') }})
    r"""The list of files used for training."""
    updated_at: int = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('updated_at') }})
    r"""The Unix timestamp (in seconds) for when the fine-tuning job was last updated."""
    validation_files: List[OpenAIFile] = dataclasses.field(metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('validation_files') }})
    r"""The list of files used for validation."""
    events: Optional[List[FineTuneEvent]] = dataclasses.field(default=None, metadata={'dataclasses_json': { 'letter_case': utils.get_field_name('events'), 'exclude': lambda f: f is None }})
    r"""The list of events that have been observed in the lifecycle of the FineTune job."""
    

